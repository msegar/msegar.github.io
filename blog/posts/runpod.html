<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embedding 500K Medical Texts with PubMedBERT on RunPod - Matt Segar</title>

    <!-- Favicon -->
    <link rel="icon" href="assets/favicon_io/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="assets/favicon_io/favicon.ico" type="image/x-icon">
    
    <!-- Primary Meta Tags -->
    <meta name="description" content="A practical guide to running large-scale biomedical embeddings on GPU.">
    <meta name="author" content="Matt Segar">
    <meta name="keywords" content="runpod, pubmedbert, embedding, medical, texts">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://segar.me/blog/posts/runpod.html">
    <meta property="og:title" content="Embedding 500K Medical Texts with PubMedBERT on RunPod - Matt Segar">
    <meta property="og:description" content="A practical guide to running large-scale biomedical embeddings on GPU.">
    <meta property="og:image" content="../assets/images/runpod.png">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://segar.me/blog/posts/runpod.html">
    <meta property="twitter:title" content="Embedding 500K Medical Texts with PubMedBERT on RunPod - Matt Segar">
    <meta property="twitter:description" content="A practical guide to running large-scale biomedical embeddings on GPU.">
    <meta property="twitter:image" content="../assets/images/runpod.png">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://segar.me/blog/posts/runpod.html">
    
    <!-- Structured Data for SEO -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Embedding 500K Medical Texts with PubMedBERT on RunPod",
      "description": "A practical guide to running large-scale biomedical embeddings on GPU.",
      "image": "../assets/images/runpod.png",
      "author": {
        "@type": "Person",
        "name": "Matt Segar"
      },
      "publisher": {
        "@type": "Person",
        "name": "Matt Segar",
        "logo": {
          "@type": "ImageObject",
          "url": "https://segar.me/assets/images/logo_name.webp"
        }
      },
      "datePublished": "2026-01-18T00:00:00.000Z",
      "dateModified": "2026-01-18T00:00:00.000Z",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://segar.me/blog/posts/runpod.html"
      },
      "keywords": "runpod, pubmedbert, embedding, medical, texts"
    }
    </script>
    
    <link rel="stylesheet" href="../../assets/css/normalize.css">
    <link rel="stylesheet" href="../../assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Open+Sans:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        /* Blog post specific styles */
        .post-header {
            margin-bottom: 40px;
        }
        
        .post-title {
            font-size: 2.4rem;
            margin-bottom: 10px;
        }
        
        .post-meta {
            font-size: 0.9rem;
            color: var(--light-text);
            margin-bottom: 20px;
        }
        
        .post-content {
            font-size: 1.1rem;
            line-height: 1.8;
            max-width: 800px;
            margin: 0 auto;
        }
        
        .post-content h2 {
            margin-top: 40px;
            margin-bottom: 20px;
        }
        
        .post-content p {
            margin-bottom: 20px;
        }
        
        .post-content img {
            max-width: 100%;
            height: auto;
            margin: 30px 0;
            border-radius: 5px;
        }
        
        .post-content pre {
            background-color: var(--light-background);
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        .post-content code {
            font-family: monospace;
            background-color: var(--light-background);
            padding: 2px 5px;
            border-radius: 3px;
        }
        
        .post-nav {
            display: flex;
            justify-content: space-between;
            margin-top: 50px;
            padding-top: 30px;
            border-top: 1px solid var(--border-color);
        }
        
        .post-nav a {
            padding: 10px 15px;
            border: 1px solid var(--border-color);
            border-radius: 3px;
            text-decoration: none;
        }

    </style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-N9RJDGM699"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-N9RJDGM699');
    </script>
</head>
<body>
    <header>
        <div class="container">
            <div class="logo">
                <a href="index.html" class="logo-link">
                    <img src="../../assets/images/logo_color.webp" alt="Matt Segar Logo" class="logo-image light-logo" width="40" height="40">
                    <img src="../../assets/images/logo_dark.webp" alt="Matt Segar Logo" class="logo-image dark-logo" width="40" height="40">
                    <h1>Matt Segar</h1>
                </a>
            </div>
            <nav>
                <ul>
                    <li><a href="../../index.html">Home</a></li>
                    <li><a href="../../cv.html">CV</a></li>
                    <li><a href="../index.html" class="active">Blog</a></li>
                    <li class="dropdown">
                        <a class="nav-link">Innovations</a>
                        <div class="dropdown-content">
                            <a href="../../book.html">ML Book</a>
                            <a href="../../job-search-book.html">Physician's Job Search Playbook</a>
                            <a href="../../valabformatter/index.html">CPRS Lab Formatter</a>
                        </div>
                    </li>
                    <li class="theme-toggle">
                        <button id="theme-toggle-btn" aria-label="Toggle dark mode">
                            <span class="light-icon">‚òÄÔ∏è</span>
                            <span class="dark-icon">üåô</span>
                        </button>
                    </li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <section class="container">
            <article class="post-content">
                <header class="post-header">
                    <h1 class="post-title">Embedding 500K Medical Texts with PubMedBERT on RunPod</h1>
                    <div class="post-meta">
                        <time datetime="2026-01-18T00:00:00.000Z">January 18, 2026</time>
                    </div>
                </header>
                
                <div class="post-body">
                    <p>I recently needed to generate embeddings for over half a million paragraphs of medical/biomedical text. Running this on my laptop would have taken days, so I turned to RunPod for on-demand GPU compute. Here&#39;s what I learned, including the gotchas that cost me hours of debugging.</p>
<h2 id="why-pubmedbert">Why PubMedBERT?</h2>
<p>When embedding domain-specific text, generic models like <code>all-MiniLM-L6-v2</code> leave performance on the table. <strong>PubMedBERT</strong> (<code>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext</code>) is pre-trained exclusively on biomedical literature‚ÄîPubMed abstracts and full-text articles from PubMed Central.</p>
<h3 id="what-makes-it-different">What Makes It Different</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Generic BERT</th>
<th>PubMedBERT</th>
</tr>
</thead>
<tbody><tr>
<td>Training data</td>
<td>Wikipedia, books</td>
<td>PubMed abstracts + PMC full texts</td>
</tr>
<tr>
<td>Vocabulary</td>
<td>General English</td>
<td>Biomedical terminology</td>
</tr>
<tr>
<td>Domain terms</td>
<td>Often split into subwords</td>
<td>Preserved as single tokens</td>
</tr>
</tbody></table>
<p>A term like &quot;cardiomyopathy&quot; might be tokenized as <code>[&#39;card&#39;, &#39;##io&#39;, &#39;##my&#39;, &#39;##op&#39;, &#39;##athy&#39;]</code> by generic BERT, but PubMedBERT recognizes it as a single token. This matters for downstream tasks like semantic search, clustering, and classification.</p>
<h3 id="when-to-use-pubmedbert">When to Use PubMedBERT</h3>
<ul>
<li>Medical literature search and retrieval</li>
<li>Clinical note analysis</li>
<li>Drug/disease relationship extraction</li>
<li>Biomedical research clustering</li>
<li>Grant abstract similarity matching</li>
</ul>
<p>The model outputs 768-dimensional vectors, which is standard for BERT-base models. You can store these in PostgreSQL with pgvector, Pinecone, Qdrant, or any vector database.</p>
<h2 id="why-runpod">Why RunPod?</h2>
<p>I needed to embed ~500,000 text passages. On CPU, this would take 20+ hours. On an RTX 4090, it takes under 2 hours.</p>
<p>RunPod offers:</p>
<ul>
<li><strong>Pay-per-use GPU instances</strong> ‚Äî no commitment, pay by the minute</li>
<li><strong>Pre-configured PyTorch templates</strong> ‚Äî CUDA drivers pre-installed</li>
<li><strong>SSH access</strong> ‚Äî work in your familiar terminal environment</li>
<li><strong>Persistent volumes</strong> ‚Äî keep model weights cached between sessions</li>
</ul>
<h3 id="cost-breakdown">Cost Breakdown</h3>
<p>For my 500K embedding job:</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>Hourly Rate</th>
<th>Total Time</th>
<th>Total Cost</th>
</tr>
</thead>
<tbody><tr>
<td>RTX 4090 (24GB)</td>
<td>~$0.44/hr</td>
<td>~1.5 hrs</td>
<td>~$0.66</td>
</tr>
<tr>
<td>RTX 3090 (24GB)</td>
<td>~$0.31/hr</td>
<td>~2 hrs</td>
<td>~$0.62</td>
</tr>
<tr>
<td>A100 40GB</td>
<td>~$1.09/hr</td>
<td>~1 hr</td>
<td>~$1.09</td>
</tr>
</tbody></table>
<p>The RTX 4090 hits the sweet spot for this workload‚Äîfast enough that you don&#39;t need A100 pricing.</p>
<h2 id="setting-up-runpod">Setting Up RunPod</h2>
<h3 id="1-launch-an-instance">1. Launch an Instance</h3>
<ol>
<li>Create a RunPod account and add credits</li>
<li>Go to <strong>Pods</strong> ‚Üí <strong>Deploy</strong></li>
<li>Select <strong>RTX 4090</strong> (24GB VRAM)</li>
<li>Choose template: <strong>RunPod PyTorch 2.1</strong></li>
<li>Set volume size: <strong>20GB</strong> (for model cache)</li>
<li>Deploy</li>
</ol>
<p>The instance takes 1-2 minutes to initialize.</p>
<h3 id="2-connect-via-ssh">2. Connect via SSH</h3>
<p>Click <strong>Connect</strong> in the RunPod dashboard to get your SSH command:</p>
<pre><code class="language-bash">ssh root@&lt;IP&gt; -p &lt;PORT&gt;
</code></pre>
<p>Or use VS Code&#39;s Remote-SSH extension for a better experience.</p>
<h3 id="3-install-dependencies">3. Install Dependencies</h3>
<pre><code class="language-bash"># Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126

# Install sentence-transformers
pip install sentence-transformers psycopg2-binary tqdm
</code></pre>
<h3 id="4-verify-gpu-access">4. Verify GPU Access</h3>
<pre><code class="language-bash">python -c &quot;import torch; print(f&#39;CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0)}&#39;)&quot;
</code></pre>
<p>Expected output:</p>
<pre><code>CUDA: True, GPU: NVIDIA GeForce RTX 4090
</code></pre>
<h2 id="the-embedding-script">The Embedding Script</h2>
<p>Here&#39;s a minimal example that batches efficiently:</p>
<pre><code class="language-python">from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import numpy as np

# Load model (downloads ~500MB on first run)
model = SentenceTransformer(&#39;microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&#39;)

def embed_texts(texts: list[str], batch_size: int = 64) -&gt; np.ndarray:
    &quot;&quot;&quot;Embed texts in batches with progress bar.&quot;&quot;&quot;
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc=&quot;Embedding&quot;):
        batch = texts[i:i + batch_size]
        batch_embeddings = model.encode(
            batch,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        embeddings.append(batch_embeddings)

    return np.vstack(embeddings)

# Example usage
texts = [&quot;Patient presents with acute myocardial infarction...&quot;, ...]
vectors = embed_texts(texts, batch_size=64)
print(f&quot;Shape: {vectors.shape}&quot;)  # (n_texts, 768)
</code></pre>
<h3 id="optimal-batch-size">Optimal Batch Size</h3>
<table>
<thead>
<tr>
<th>GPU VRAM</th>
<th>Recommended Batch Size</th>
</tr>
</thead>
<tbody><tr>
<td>8GB</td>
<td>16</td>
</tr>
<tr>
<td>16GB</td>
<td>32</td>
</tr>
<tr>
<td>24GB</td>
<td>64</td>
</tr>
<tr>
<td>40GB+</td>
<td>128</td>
</tr>
</tbody></table>
<p>If you hit CUDA OOM errors, reduce batch size.</p>
<h2 id="gotchas-that-cost-me-hours">Gotchas That Cost Me Hours</h2>
<h3 id="1-pytorch-version-requirements-cve-2025-32434">1. PyTorch Version Requirements (CVE-2025-32434)</h3>
<p><strong>The error:</strong></p>
<pre><code>ValueError: Due to a serious vulnerability issue in `torch.load`, even with
`weights_only=True`, we now require users to upgrade torch to at least v2.6
</code></pre>
<p><strong>The problem:</strong> Recent versions of <code>transformers</code> and <code>sentence-transformers</code> require PyTorch 2.6+ due to a security vulnerability. The older CUDA indexes (<code>cu121</code>, <code>cu124</code>) only have PyTorch 2.5.x.</p>
<p><strong>The fix:</strong> Use the <code>cu126</code> (or <code>cu130</code>) index:</p>
<pre><code class="language-bash">pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126
</code></pre>
<h3 id="2-ipv6-vs-ipv4-supabasecloud-database">2. IPv6 vs IPv4 (Supabase/Cloud Database)</h3>
<p><strong>The error:</strong></p>
<pre><code>psycopg2.OperationalError: connection to server at &quot;db.xxx.supabase.co&quot;
(2600:...), port 5432 failed: Network is unreachable
</code></pre>
<p><strong>The problem:</strong> RunPod instances are IPv4-only, but many cloud database providers (Supabase, some AWS RDS configs) use IPv6 for direct connections.</p>
<p><strong>The fix:</strong> Use a connection pooler that supports IPv4. For Supabase, this means using the &quot;Shared Pooler&quot; connection string instead of the direct connection:</p>
<pre><code># Instead of this (IPv6):
postgresql://postgres:pass@db.xxx.supabase.co:5432/postgres

# Use this (IPv4 compatible):
postgresql://postgres.xxx:pass@aws-0-us-east-1.pooler.supabase.com:6543/postgres
</code></pre>
<h3 id="3-statement-timeouts-on-large-queries">3. Statement Timeouts on Large Queries</h3>
<p><strong>The error:</strong></p>
<pre><code>psycopg2.errors.QueryCanceled: canceling statement due to statement timeout
</code></pre>
<p><strong>The problem:</strong> Connection poolers often have default statement timeouts (e.g., 60 seconds). Querying 500K rows exceeds this.</p>
<p><strong>The fix:</strong> Set a longer timeout at session start:</p>
<pre><code class="language-python">cursor.execute(&quot;SET statement_timeout = &#39;300s&#39;&quot;)  # 5 minutes
</code></pre>
<h3 id="4-ssh-disconnects-kill-your-job">4. SSH Disconnects Kill Your Job</h3>
<p><strong>The problem:</strong> Long-running jobs die when your SSH connection drops.</p>
<p><strong>The fix:</strong> Use <code>tmux</code> to persist sessions:</p>
<pre><code class="language-bash"># Install tmux
apt-get update &amp;&amp; apt-get install -y tmux

# Start a named session
tmux new -s embed

# Run your script
python embed.py

# Detach: Ctrl+B, then D
# Reattach later: tmux attach -t embed
</code></pre>
<h3 id="5-model-download-timeouts">5. Model Download Timeouts</h3>
<p><strong>The problem:</strong> The first model load downloads ~500MB from HuggingFace, which can timeout.</p>
<p><strong>The fix:</strong> Pre-download before your main script:</p>
<pre><code class="language-bash">python -c &quot;from sentence_transformers import SentenceTransformer; SentenceTransformer(&#39;microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&#39;)&quot;
</code></pre>
<p>Or use huggingface-cli:</p>
<pre><code class="language-bash">pip install huggingface_hub
huggingface-cli download microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
</code></pre>
<h2 id="performance-tips">Performance Tips</h2>
<h3 id="1-monitor-gpu-utilization">1. Monitor GPU Utilization</h3>
<p>In a separate terminal:</p>
<pre><code class="language-bash">watch -n 1 nvidia-smi
</code></pre>
<p>You should see 80-95% GPU utilization. If it&#39;s lower, your batch size might be too small or you&#39;re bottlenecked on data loading.</p>
<h3 id="2-commit-to-database-in-batches">2. Commit to Database in Batches</h3>
<p>Don&#39;t commit after every row. Batch your database writes:</p>
<pre><code class="language-python">COMMIT_BATCH = 500
buffer = []

for text, embedding in generate_embeddings():
    buffer.append((text, embedding))

    if len(buffer) &gt;= COMMIT_BATCH:
        insert_batch(buffer)
        conn.commit()
        buffer = []

# Don&#39;t forget the final batch
if buffer:
    insert_batch(buffer)
    conn.commit()
</code></pre>
<h3 id="3-use-direct-postgresql-for-bulk-inserts">3. Use Direct PostgreSQL for Bulk Inserts</h3>
<p>ORMs and REST APIs add overhead. For bulk operations, use <code>psycopg2</code> directly with <code>execute_values</code>:</p>
<pre><code class="language-python">from psycopg2.extras import execute_values

def insert_embeddings(cursor, data):
    execute_values(
        cursor,
        &quot;&quot;&quot;
        INSERT INTO embeddings (id, vector)
        VALUES %s
        ON CONFLICT (id) DO NOTHING
        &quot;&quot;&quot;,
        data
    )
</code></pre>
<h2 id="final-numbers">Final Numbers</h2>
<p>For my 500K medical text embedding job:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Total texts</td>
<td>506,205</td>
</tr>
<tr>
<td>Model</td>
<td>PubMedBERT</td>
</tr>
<tr>
<td>Embedding dimension</td>
<td>768</td>
</tr>
<tr>
<td>GPU</td>
<td>RTX 4090</td>
</tr>
<tr>
<td>Batch size</td>
<td>64</td>
</tr>
<tr>
<td>Total time</td>
<td>~1.5 hours</td>
</tr>
<tr>
<td>Total cost</td>
<td>~$0.66</td>
</tr>
<tr>
<td>Throughput</td>
<td>~5,600 texts/minute</td>
</tr>
</tbody></table>
<h2 id="conclusion">Conclusion</h2>
<p>RunPod makes it trivially easy to spin up GPU compute for batch jobs. The key lessons:</p>
<ol>
<li><strong>Use domain-specific models</strong> ‚Äî PubMedBERT significantly outperforms generic models on biomedical text</li>
<li><strong>Watch your PyTorch version</strong> ‚Äî security patches broke older CUDA indexes</li>
<li><strong>Check IPv4/IPv6 compatibility</strong> ‚Äî cloud instances often don&#39;t support IPv6</li>
<li><strong>Use tmux</strong> ‚Äî don&#39;t let SSH disconnects kill hours of work</li>
<li><strong>Batch everything</strong> ‚Äî GPU encoding, database commits, all of it</li>
</ol>
<p>The whole job cost less than a dollar and took under two hours. Compare that to days on CPU or hundreds of dollars for always-on GPU instances.</p>
<hr>
<p><em>Have questions or found other gotchas? Feel free to reach out.</em></p>

                </div>
                
                <div class="post-nav">
                    <a href="../index.html">‚Üê Back to Blog</a>
                </div>
            </article>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; <script>document.write(new Date().getFullYear())</script> Dr. Matt Segar. All rights reserved.</p>
            <p><a href="/sitemap.xml">Sitemap</a> | <a href="/privacy-policy.html">Privacy Policy</a></p>
        </div>
    </footer>

    <!-- External JS file for theme toggle -->
    <script src="../../assets/js/theme-toggle.js"></script>
</body>
</html>